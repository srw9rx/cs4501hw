{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sophia Walton, srw9rx\n",
    "\n",
    "14 november 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 04 Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Suggestions and Requirements\n",
    "\n",
    "- Search the keyword TODO will help you quickly locate the questions in this assignment.\n",
    "- Please **keep** all the outputs in your submission. Our TA is not required to run the code for grading. \n",
    "- Training the bug-free version of RNN LM (section 2) may need 30 minutes or longer, depending your local hardware. So when you implement/debug your code, you can create a small training file (e.g., only use the first 1K sentences) for the purpose of testing.\n",
    "\n",
    "\n",
    "First, let's download the data for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...\n",
      "Decompressing the file ...\n",
      "Archive:  arxiv.zip\n",
      "   creating: arxiv/\n",
      "  inflating: arxiv/trn-arxiv.txt     \n",
      "  inflating: arxiv/dev-arxiv.txt     \n",
      "Read 63259 sentences from the training set\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "\n",
    "import urllib.request\n",
    "from os.path import isfile\n",
    "if not isfile(\"arxiv.zip\"):\n",
    "    url = \"https://yangfengji.net/uva-nlp-course/data/arxiv.zip\"\n",
    "    print(\"Downloading ...\")\n",
    "    filename, headers = urllib.request.urlretrieve(url, filename=\"arxiv.zip\")\n",
    "\n",
    "print(\"Decompressing the file ...\")\n",
    "!unzip arxiv.zip\n",
    "\n",
    "sents = open(\"arxiv/trn-arxiv.txt\").read().split(\"\\n\")\n",
    "print(\"Read {} sentences from the training set\".format(len(sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistical Language Modeling\n",
    "\n",
    "The following homework is designed based on the demo code of statistical language modeling. In this homework, you will be asked to implemented two additional components besides the demo code\n",
    "\n",
    "1. Parameter estimation with discounting\n",
    "2. Perplexity calculation\n",
    "\n",
    "Our implementation starts from the following demo code as we used in our class lecture, which will save us some time to implement the `logprob()` function and the `generate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import log2, pow\n",
    "from numpy.random import choice\n",
    "\n",
    "# This is the same model used in our class lecture demo code\n",
    "\n",
    "class BigramLM(object):\n",
    "    def __init__(self):\n",
    "        self.vocab = {\"<start>\":0, \"<end>\":1}\n",
    "        self.model = {}\n",
    "        self.tok_counter = '__total__'\n",
    "\n",
    "    def build(self, fname):\n",
    "        \"\"\" Build a Bigram LM\n",
    "        \"\"\"\n",
    "        fin = open(fname)\n",
    "        for line in fin:\n",
    "            tokens = line.strip().split()\n",
    "            L = len(tokens)\n",
    "            for i in range(1, L):\n",
    "                prev_tok = tokens[i-1]\n",
    "                curr_tok = tokens[i]\n",
    "                if curr_tok not in self.vocab:\n",
    "                    self.vocab.update({curr_tok : len(self.vocab)})\n",
    "                try:\n",
    "                    self.model[prev_tok][curr_tok] += 1.0\n",
    "                except KeyError:\n",
    "                    self.model[prev_tok] = defaultdict(float)\n",
    "                    self.model[prev_tok][curr_tok] += 1.0\n",
    "                self.model[prev_tok][self.tok_counter] += 1.0\n",
    "        # Normalization\n",
    "        for (prev_tok, dct) in self.model.items():\n",
    "            for (curr_tok, val) in self.model[prev_tok].items():\n",
    "                if curr_tok != self.tok_counter: # to avoid normalizing the counter token\n",
    "                    self.model[prev_tok][curr_tok] /= self.model[prev_tok][self.tok_counter]\n",
    "        print(\"Done with modeling building\\nVocab size = {}\".format(len(self.vocab)))\n",
    "            \n",
    "\n",
    "    def logprob(self, text):\n",
    "        \"\"\" Evaluate a given text\n",
    "        \"\"\"\n",
    "        tokens = text.strip().split()\n",
    "        L = len(tokens)\n",
    "        logprob = 0.0\n",
    "        for i in range(1, L):\n",
    "            prev_tok = tokens[i-1]\n",
    "            curr_tok = tokens[i]\n",
    "            if prev_tok not in self.vocab:\n",
    "                prev_tok = 'UNK'\n",
    "            if curr_tok not in self.vocab:\n",
    "                curr_tok = 'UNK'\n",
    "            try:\n",
    "                logprob += log2(self.model[prev_tok][curr_tok])\n",
    "            except ValueError:\n",
    "                print(\"Missing word pair: {} -> {} from the probability table\".format(prev_tok, curr_tok))\n",
    "                logprob += -1000 # A large number, technically this should be infty\n",
    "        return logprob\n",
    "\n",
    "\n",
    "    def generate(self, method=\"random\", length=20):\n",
    "        \"\"\" Random sampling words from this model for generation\n",
    "        \"\"\"\n",
    "        text = []\n",
    "        prev_tok = \"<start>\"\n",
    "        text.append(prev_tok)\n",
    "        while (prev_tok != \"<end>\") and (len(text) <= length):\n",
    "            tokens, probs = [], []\n",
    "            # The following for loop is time-consuming\n",
    "            # For large-scale text generation, a pre-processing may be necessary \n",
    "            for (tok, prob) in self.model[prev_tok].items():\n",
    "                if tok != self.tok_counter:\n",
    "                    tokens.append(tok)\n",
    "                    probs.append(prob)\n",
    "                \n",
    "                #print(prev_tok, tokens[probs.index(max(probs))])\n",
    "            widx = choice(len(probs), 1, p=probs)[0]\n",
    "            prev_tok = tokens[widx]\n",
    "            text.append(prev_tok)\n",
    "        text = \" \".join(text)\n",
    "        return text            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Discounting (3 points)\n",
    "\n",
    "$$P(x_t\\mid x_{t-1}) = \\frac{c(x_{t-1},x_t) + \\alpha}{c(x_{t-1}) + V\\alpha}$$\n",
    "where $V$ is the vocabulary size and $\\alpha$ is the hyper-parameter of discounting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class `BigramLM_Discounted` is inherited from the class `BigramLM`, which is implemented in the demo code for our lecture. \n",
    "\n",
    "In the following code block, please implement the discounting technique as shown in the above equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM_Discounted(BigramLM):\n",
    "    '''\n",
    "    The logprob() function and the generate() function will be inherited from the BigramLM class. \n",
    "    The only thing left here is the build() function for building language models with discouting. \n",
    "    '''\n",
    "    def build(self, fname, alpha=0.1):\n",
    "        fin = open(fname)\n",
    "        for line in fin:\n",
    "            tokens = line.strip().split()\n",
    "            L = len(tokens)\n",
    "            for i in range(1, L):\n",
    "                prev_tok = tokens[i-1]\n",
    "                curr_tok = tokens[i]\n",
    "                if curr_tok not in self.vocab:\n",
    "                    self.vocab.update({curr_tok : len(self.vocab)})\n",
    "                try:\n",
    "                    self.model[prev_tok][curr_tok] += 1.0\n",
    "                except KeyError:\n",
    "                    self.model[prev_tok] = defaultdict(float)\n",
    "                    self.model[prev_tok][curr_tok] += (1.0+alpha) #add 1+alpha to value for having alpha in each one\n",
    "                    self.model[prev_tok][self.tok_counter]+=alpha #for each first item, add alpha value in\n",
    "                self.model[prev_tok][self.tok_counter] += 1.0\n",
    "        # Normalization\n",
    "        for (prev_tok, dct) in self.model.items():\n",
    "            for (curr_tok, val) in self.model[prev_tok].items():\n",
    "                if curr_tok != self.tok_counter: # to avoid normalizing the counter token\n",
    "                    self.model[prev_tok][curr_tok] /= (self.model[prev_tok][self.tok_counter]) #+(len(self.model[prev_tok].keys())*alpha))\n",
    "        totalprob=sum(self.model[prev_tok].values())-self.model[prev_tok][self.tok_counter]\n",
    "        for (prev_tok, dct) in self.model.items():\n",
    "            for (curr_tok, val) in self.model[prev_tok].items():\n",
    "                if curr_tok != self.tok_counter: # to avoid normalizing the counter token\n",
    "                    self.model[prev_tok][curr_tok] /= totalprob\n",
    "        print(\"Done with modeling building\\nVocab size = {}\".format(len(self.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to make sure your implementation is executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with modeling building\n",
      "Vocab size = 12455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<start> lead2gold searches gradually increasing pressure but are evaluated our model that reported to produce reasonable accuracy of domains such formulation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dis = BigramLM_Discounted()\n",
    "bigram_dis.build(\"arxiv/trn-arxiv.txt\")\n",
    "bigram_dis.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of comparison, let's also build a language model without discounting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with modeling building\n",
      "Vocab size = 12455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<start> the text generation time <end>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = BigramLM()\n",
    "bigram.build(\"arxiv/trn-arxiv.txt\")\n",
    "bigram.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Edge Cases (2 points)\n",
    "\n",
    "Please design a edge case to demonstrate the value of the discounting techniques. Each edge case will be a sentence, which can demonstrate the difference between a language model with and without discounting. \n",
    "\n",
    "The requirement of forming an edge case\n",
    "\n",
    "- it needs to be an English word sequence\n",
    "- there is a significant difference between log probabilities given by the model with and without discounting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findedgecase(text):\n",
    "    with open(text) as file:\n",
    "        probnorm = 1.0\n",
    "        probdis = 1.0\n",
    "        maxval = 0.0\n",
    "        maxsent = \"\"\n",
    "        nextline = file.readline()\n",
    "        for i in range(0, 63259, 1): \n",
    "            line = \"\"\n",
    "            try: \n",
    "                line = nextline\n",
    "                probnorm = bigram.logprob(line)\n",
    "                probdis = bigram_dis.logprob(line)\n",
    "                #print(probnorm,probdis)\n",
    "                #print(line)\n",
    "                nextline = file.readline()\n",
    "                if abs(probnorm-probdis)>maxval:\n",
    "                    maxsent = line\n",
    "                    maxval = abs(probnorm-probdis)\n",
    "            except ZeroDivisionError:\n",
    "                probnorm = 1.0\n",
    "                probdis = 1.0\n",
    "                nextline = file.readline()\n",
    "        print(maxval)\n",
    "        print(maxsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6965051163887779\n",
      "<start> simple review by linguist citing many articles by physicists quantitative methods agent based computer simulations language dynamics language typology historical linguistics <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textsent = findedgecase(\"arxiv/trn-arxiv.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text = <start> simple review by linguist citing many articles by physicists quantitative methods agent based computer simulations language dynamics language typology historical linguistics <end>\n",
      " Log prob on the original model -166.6402\n",
      " Log prob on the model with discounting -164.9436915775177\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# TODO: replace the following None with an edge case that you defined\n",
    "text = \"<start> simple review by linguist citing many articles by physicists quantitative methods agent based computer simulations language dynamics language typology historical linguistics <end>\"\n",
    "# =====================================\n",
    "logprob_bigram = bigram.logprob(text)\n",
    "logprob_bigram_dis = bigram_dis.logprob(text)\n",
    "print(\"Text = {}\\n Log prob on the original model {:.4f}\\n Log prob on the model with discounting {}\".format(text, logprob_bigram, logprob_bigram_dis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a tokenization function for language modeling. Particularly, this tokenization function not only converts texts into a list of tokens, but also handle the padding operation, which is critical for mini-batch training in neural network modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"UNK\"))\n",
    "tokenizer.add_special_tokens(['<pad>', 'UNK'])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordLevelTrainer()\n",
    "tokenizer.train(['arxiv/trn-arxiv.txt'], trainer)\n",
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id('<pad>'), pad_token='<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 13, 13, 1628, 137, 13, 13, 20, 4623, 13]\n",
      "0\n",
      "12376\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you ?\")\n",
    "print(output.ids)\n",
    "print(tokenizer.token_to_id('<pad>'))\n",
    "print(tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset iterators for training and evaluation. The following code will convert each set (training or development) into a list of mini batches. You can change the mini-batch size with the parameter `batch_size` in the `DataLoader` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-87fef769e91ff912\n",
      "Reusing dataset text (/Users/sophiawalton/.cache/huggingface/datasets/text/default-87fef769e91ff912/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9f4059047e455e89b0b3954230f758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch.utils\n",
    "dataset = load_dataset('text', data_files={'train': 'arxiv/trn-arxiv.txt', 'dev': 'arxiv/dev-arxiv.txt'})\n",
    "\n",
    "def collate(examples):\n",
    "    texts = [ex['text'] for ex in examples]\n",
    "    return torch.LongTensor([text.ids for text in tokenizer.encode_batch(texts)])\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=32, collate_fn=collate)\n",
    "dev_iter = torch.utils.data.DataLoader(dataset[\"dev\"], batch_size=32, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will do two things for you:\n",
    "\n",
    "- Load the necessary packages for neural network modeling from PyTorch\n",
    "- Define the random seeds: by default Python and PyTorch will use a different random seed each time you run the code. Creating some randomness is good in general, but really not helpful when you debug the code. So, having a pre-defined random seed will make sure your code starts from the exactly same point everytime, which will make it relatively easier to identify any potential bugs. For example, the following code should print exactly the following tensor every time you run it\n",
    "\n",
    "> tensor([-2.0157,  2.0106,  0.0583,  0.0656, -1.6534])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0157,  2.0106,  0.0583,  0.0656, -1.6534])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm\n",
    "import torch.nn as nn\n",
    "\n",
    "## Random seeds, to make the results reproducible\n",
    "seed = 49\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(torch.randn(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Neural Network Implementation (4 points)\n",
    "\n",
    "In the following class `SimpleRNN`, you need to complete two parts of the code\n",
    "\n",
    "- In the **initialization function**, define the following three components. For each component, make sure you get the dimensionality right\n",
    " - [Word embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    " - [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) as the nonlinear transition function\n",
    " - [a fully connected layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) for computing the logit\n",
    "- In the **forward function**, define the LSTM language model that can read a mini-batch of texts and compute the logits of word predictions\n",
    " - In the definition of a softmax classifier\n",
    " $$p(x_{t+1}\\mid x_{1:t}) \\approx \\exp(U\\cdot h_{t})$$\n",
    " `logit` is defined as $U\\cdot h_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import magnum\n",
    "\n",
    "#import long\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, input_dim=16, hidden_dim=16, num_layers=1, pad_idx=None):\n",
    "        ''' Initialization function\n",
    "        \n",
    "        vocab_size - vocab size\n",
    "        input_dim - word embedding dimension (w)\n",
    "        hidden_dim - dimension of hidden states (h)\n",
    "        num_layers - number of LSTM layers (> 2: stacked LSTM)\n",
    "        pad_idx - padding token index\n",
    "        '''\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        self.vocab_size = vocab_size # Vocab size\n",
    "        self.embed_size = input_dim # Word embedding dimension\n",
    "        self.hidden_dim = hidden_dim # Hidden dimension\n",
    "        self.num_layers = num_layers # Number of layers\n",
    "        #word embedding layer - padding = none\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embed_size,padding_idx=pad_idx)\n",
    "        #LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=self.embed_size,hidden_size=self.hidden_dim,num_layers=self.num_layers,batch_first=True)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "      \n",
    "    def forward(self, input):\n",
    "        embeds = self.embed(input) # B x T x E\n",
    "        batch_size = input.size(0)\n",
    "        # ======================================\n",
    "        # TODO: complete the following code for defining a RNN LM\n",
    "        lstm_out, *_ = self.lstm(embeds)\n",
    "        logits = self.fc(lstm_out) \n",
    "        # =======================================\n",
    "        return logits #dims: batch size, sequence length, vocab size\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training with Mini-batch (2 points)\n",
    "\n",
    "Please complete the following code by defining the loss function. The loss function has been defined with the [cross entropy function](), as shown in the following code \n",
    "```\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('<pad>'), reduce=True)\n",
    "```\n",
    "`ignore_index` is to make sure that the overall loss does not include the predictions on padding tokens, `reduce=True` means the returned loss from this function is the average NLL. \n",
    "\n",
    "To complete the code, you need to specify the two inputs of the loss function: `predicted` and `target`\n",
    "\n",
    "- `predicted`: the logits of the input words\n",
    "- `target`: the word indices of output words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('<pad>'), reduce=True)\n",
    "\n",
    "def batch_train(batch, model, optimizer, loss_func=loss_func, grad_clip=5.0):\n",
    "    '''Training with one mini-batch\n",
    "\n",
    "    batch - one mini-batch data\n",
    "    model - an instance of RNN LM\n",
    "    optimizer - the optimization function (e.g., SGD or Adam)\n",
    "    loss_func - the pre-defined loss function\n",
    "    grad_clip - hyper-parameter of grad clipping\n",
    "    '''\n",
    "    # Forward \n",
    "    model.train()\n",
    "    logits = model(batch)\n",
    "\n",
    "    # ============================================\n",
    "    # TODO: Define predicted and target for the following loss function\n",
    "    predicted = logits.transpose(1,2) # dims: batch size, vocab size, sequence length\n",
    "    target = batch # dims: batch size, sequence length\n",
    "    # ============================================\n",
    "    loss = loss_func(predicted, target)\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    clip_grad_norm(model.parameters(), grad_clip) # Norm  clipping\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Perplexity (3 points)\n",
    "\n",
    "For a given evaluation dataset $\\{\\bf{x}_i\\}_{i=1}^{N}$, perplexity is defined as \n",
    "$$\\text{Perplexity} = \\exp \\big(-\\frac{1}{T}\\sum_{i=1}^{N}\\sum_{t=1}\\log p(x_{i,t}\\mid x_{i,1:t-1})\\big)$$\n",
    "where $T$ is the total number of predictions that a language model makes on the evaluation data (as we explained in the lecture), $N$ is the total number of sentences in the evaluation dataset.\n",
    "\n",
    "In the following code, please implement the function that can read all the mini-batches in the deveopment set for evaluating the model performance during training. As you can find out from the next code block, this function will be used during training to measure the model performance after certain training iterations. \n",
    "\n",
    "This function needs to read all the mini-batch from the validation set, which are all in the `dev_iter` as defined before. The implementation of this function will be mostly similar to the `batch_train` function in the last code block, but without backward steps. \n",
    "\n",
    "Unlike training, to calculate the perplexity, you will need to set the `reduce` parameter in the cross entropy function to be `False`. \n",
    "\n",
    "In addition, make sure your calculation of perplexity does not include the predictions on padding tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "loss_func_2 = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('<pad>'), reduce=False)\n",
    "\n",
    "def perplexity(dev_iter, model, loss_func=loss_func_2):\n",
    "    '''\n",
    "    '''\n",
    "    model.eval() # Set in the evaluation mode\n",
    "    pplx = 0.0\n",
    "    logs = 0\n",
    "    for (b, batch) in enumerate(dev_iter):\n",
    "        logits = model(batch)\n",
    "        loss = loss_func(logits.transpose(1,2),batch)\n",
    "        pplx += loss.sum()\n",
    "        logs += torch.numel(loss)\n",
    "        \n",
    "    pplx = torch.exp(pplx/logs)\n",
    "    return pplx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Combine All Components for Training (2 points)\n",
    "\n",
    "Once you implement all the components, please run the following code and plot the perplexity curve on the development set over training. \n",
    "\n",
    "Based on our implementation, the training procedure needs to run at least five epochs to get a good result, which will take about 10 - 30 minutes on CPUs to finish. Depending the computational resources you have, it's optional to increase the number of training epochs to get even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66475541bcb14ff186dcf2d5e6f8fdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Batch Loss: 9.412986755371094, Perplexity: 122.95208740234375\n"
     ]
    }
   ],
   "source": [
    "model = SimpleRNN(tokenizer.get_vocab_size(), input_dim=16, hidden_dim=16,\n",
    "                  num_layers=1, pad_idx=tokenizer.token_to_id('<pad>'))\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "eval_step = 200\n",
    "epochs = 5\n",
    "pplx_lst = []\n",
    "for epoch in trange(epochs):\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        loss = batch_train(batch, model, optimizer)\n",
    "        if b % eval_step == 0:\n",
    "            pplx = perplexity(dev_iter, model)\n",
    "            pplx_lst.append((b,pplx))\n",
    "            print(f\"Epoch: {epoch}, Batch: {b}, Batch Loss: {loss}, Perplexity: {pplx}\", flush=True)\n",
    "                        \n",
    "# ===============================================\n",
    "# TODO: plot the perplexity curve on the development set.\n",
    "\n",
    "\n",
    "# ================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "bs = [b for b,p in pplx_lst]\n",
    "pplxs = [p for b,p in pplx_lst]\n",
    "plt.plot(bs,pplxs)\n",
    "plt.plot(bs,pplxs,'or')\n",
    "plt.show()\n",
    "# ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
